{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고자료: https://github.com/davidsandberg/facenet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "# 주성분분석(Principal Analysis)을 하기위한 라이브러리\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 파일 경로 확인용\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# .py 파일 가져오기\n",
    "# import (폴더이름.)파일이름\n",
    "import src.facenet as facenet\n",
    "import src.align.detect_face as detect_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0. 내얼굴들의 임베딩을 미리 계산(실습 1의 embeded_me)\n",
    "# jupyter notebook 페이지에 그르기\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습되어있는 모델을 로드하는 함수\n",
    "#    입력: protobuf 파일(모델이 정의되어 있음)\n",
    "#    리턴:\n",
    "#        - single_image: 영상을 입력하는 placeholder (session.run feed_dict에 사용)\n",
    "#        - embeddings: 네트워크의 출력 값, 512차원의 추출된 얼굴 특징 벡터\n",
    "\n",
    "def load_model(pb_path, image_size=(160,160)):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    single_image = tf.placeholder(tf.int32, (None,None,3))\n",
    "    float_image = tf.cast(single_image, tf.float32)\n",
    "    float_image = float_image / 255\n",
    "    batch_image = tf.expand_dims(float_image, 0)\n",
    "    resized_image = tf.image.resize(batch_image, image_size)\n",
    "    \n",
    "    phase_train = tf.placeholder_with_default(False, shape=[])\n",
    "    \n",
    "    input_map = {'image_batch':resized_image, 'phase_train':phase_train}\n",
    "    model = facenet.load_model(pb_path, input_map)\n",
    "    \n",
    "    embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "    \n",
    "    return single_image, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 경로를 입력박아 로드하고, return해주는 함수\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "    return image\n",
    "\n",
    "# 두 벡터간의 거리를 계산하는 함수\n",
    "def calc_distance(embedding1, embedding2):\n",
    "    # Euclidian distance\n",
    "    diff = np.subtract(embedding1, embedding2)\n",
    "    dist = np.sum(np.square(diff),0)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 함수정의\n",
    "# 단일 RGB영상과 pre-trained 네트워크(P/R/0 net)를 입력받아\n",
    "#   1. face detection\n",
    "#   2. crop (margin 추가)\n",
    "#   3. resize 하고,\n",
    "#  검출된 얼굴 영상들의 리스트를 반환하는 함수\n",
    "\n",
    "def crop_faces(image, pnet, rnet, onet):\n",
    "    # face detection 관현 파라미터\n",
    "    \n",
    "    minsize = 20 #minimum size of face\n",
    "    threshold = [0.6, 0.7, 0.7] # three step's threshold\n",
    "    factor = 0.709 # scale factor\n",
    "    \n",
    "    # crop 관련 파라미터\n",
    "    margin = 44 # 상하좌우 여백\n",
    "    image_size = 160 # resize(scaling) 크기\n",
    "    \n",
    "    h,w,_ = np.shape(image)\n",
    "    \n",
    "    # 얼굴 검출\n",
    "    bounding_boxes, points = detect_face.detect_face(image, minsize, pnet, rnet, onet, threshold, factor)\n",
    "    # bounding_boxes: 검출된 사각형 영역,\n",
    "    #                 [x1,y1,x2,y2,확률]로 이루어진 리스트\n",
    "    # points: 검출된 얼굴의 주요 landmark,\n",
    "    #         [x1,x2,x3,x4,x5,y1,y2,y3,y4,y5]로 이루어진 리스트\n",
    "    \n",
    "    faces = []\n",
    "    for box in bounding_boxes:\n",
    "        box = np.int32(box)\n",
    "        bb = np.zeros(4, dtype=np.int32)\n",
    "        bb[0] = np.maximum(box[0]-margin/2, 0)\n",
    "        bb[1] = np.maximum(box[1]-margin/2, 0)\n",
    "        bb[2] = np.minimum(box[2]+margin/2, w)\n",
    "        bb[3] = np.minimum(box[3]+margin/2, h)\n",
    "        cropped = image[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "        scaled = cv2.resize(cropped, (image_size, image_size), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        faces.append(scaled)\n",
    "    \n",
    "    return faces, bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model filename: ./models/20180402-114759.pb\n",
      "WARNING:tensorflow:From C:\\mip2019\\src\\facenet.py:371: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "WARNING:tensorflow:From C:\\Users\\wsx13\\Anaconda3\\envs\\mip2019\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\mip2019\\src\\align\\detect_face.py:213: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "## 모델 및 세션 선언\n",
    "tf.reset_default_graph()\n",
    "single_image, embeddings = load_model(\"./models/20180402-114759.pb\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "## 학습되어 있는 detection 모델 로드\n",
    "pnet, rnet, onet = detect_face.create_mtcnn(sess, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 512)\n"
     ]
    }
   ],
   "source": [
    "## 내 얼굴의 특징벡터(embedding)추출\n",
    "\n",
    "path_me = glob.glob(\"./project1/data/faces/me/*\")\n",
    "\n",
    "embed_me = []\n",
    "\n",
    "for path in path_me:\n",
    "    img = load_image(path)\n",
    "    result = sess.run(embeddings, feed_dict={single_image:img})\n",
    "    result = result[0]\n",
    "    embed_me.append(result)\n",
    "    \n",
    "embed_me = np.array(embed_me)\n",
    "print(embed_me.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. 동영상 로드\n",
    "cap = cv2.VideoCapture(\"./project1/data/faces/video/people.mp4\")\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    frame = cv2.resize(frame,(400,225))\n",
    "    \n",
    "    # 2. 프레임 별로 얼굴 검출\n",
    "    faces, bounding_boxes = crop_faces(frame, pnet, rnet, onet)\n",
    "    number_f = len(faces)\n",
    "    \n",
    "    squared_img = frame.copy()\n",
    "    for box in bounding_boxes:\n",
    "        box = np.int32(box)\n",
    "        p1 = (box[0], box[1])\n",
    "        p2 = (box[2], box[3])\n",
    "        # if 내얼굴 == 파란색\n",
    "        # else == 빨간색\n",
    "        cv2.rectangle(squared_img, p1, p2, color=(255,0,0))\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('detected', squared_img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
